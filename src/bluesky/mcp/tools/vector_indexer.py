"""
NECB Vector Index Builder

Builds ChromaDB semantic search index from NECB SQLite database.
Uses GPU-accelerated embedding model for high-quality vectors.

Architecture:
- Loads all sections and tables from necb.db
- Generates 1024-dim embeddings using e5-large-v2 (GPU) or 384-dim with MiniLM (CPU)
- Stores in ChromaDB with metadata for filtering
- Persists index to disk for fast loading

Index structure:
- Collection name: "necb_{vintage}" (e.g., "necb_2017")
- Documents: Section content + table content
- Metadata: {vintage, content_type, section_number, page_number, table_number, title}
- Embeddings: Generated by optimal model (GPU-preferred)
"""

import logging
import sqlite3
import json
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass

import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer

from bluesky.mcp.tools.model_config import get_optimal_embedding_model

logger = logging.getLogger(__name__)


@dataclass
class NECBDocument:
    """Document to be indexed in ChromaDB."""

    id: str  # Unique ID for ChromaDB
    content: str  # Text to embed
    metadata: Dict  # Filterable metadata
    embedding: Optional[List[float]] = None


class NECBVectorIndexer:
    """Build and manage NECB semantic search index."""

    def __init__(
        self,
        db_path: Path,
        chroma_path: Path,
        model: Optional[SentenceTransformer] = None,
        batch_size: int = 32,
    ):
        """
        Initialize vector indexer.

        Args:
            db_path: Path to necb.db SQLite database
            chroma_path: Path to ChromaDB storage directory
            model: Pre-loaded embedding model (optional)
            batch_size: Batch size for embedding generation (GPU: 32-64, CPU: 8-16)
        """
        self.db_path = db_path
        self.chroma_path = chroma_path
        self.batch_size = batch_size

        # Load embedding model
        if model is None:
            logger.info("Loading optimal embedding model...")
            self.model, self.model_config = get_optimal_embedding_model()
            logger.info(
                f"Using {self.model_config['model_name']} on {self.model_config['device']}"
            )
        else:
            self.model = model
            self.model_config = {"model_name": "custom", "device": "unknown"}

        # Initialize ChromaDB
        self.chroma_path.mkdir(parents=True, exist_ok=True)
        self.client = chromadb.PersistentClient(
            path=str(chroma_path),
            settings=Settings(anonymized_telemetry=False),
        )

        logger.info(f"ChromaDB initialized at {chroma_path}")

    def build_index(
        self, vintages: Optional[List[str]] = None, force_rebuild: bool = False
    ) -> Dict[str, int]:
        """
        Build vector index for specified NECB vintages.

        Args:
            vintages: List of vintages to index (default: all - 2011, 2015, 2017, 2020)
            force_rebuild: Delete existing collections and rebuild

        Returns:
            Dict with counts: {vintage: document_count}
        """
        if vintages is None:
            vintages = ["2011", "2015", "2017", "2020"]

        logger.info(f"Building vector index for NECB vintages: {vintages}")

        results = {}
        for vintage in vintages:
            count = self._build_vintage_index(vintage, force_rebuild)
            results[vintage] = count

        logger.info(f"✓ Index build complete: {results}")
        return results

    def _build_vintage_index(self, vintage: str, force_rebuild: bool) -> int:
        """Build index for a single vintage."""
        collection_name = f"necb_{vintage}"

        # Check if collection exists
        if force_rebuild:
            try:
                self.client.delete_collection(collection_name)
                logger.info(f"Deleted existing collection: {collection_name}")
            except Exception:
                pass  # Collection doesn't exist

        # Create or get collection
        collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={
                "vintage": vintage,
                "model": self.model_config["model_name"],
                "dimensions": self.model_config.get("dimensions", 0),
            },
        )

        # Check if already populated
        if collection.count() > 0 and not force_rebuild:
            logger.info(
                f"Collection {collection_name} already has {collection.count()} documents (skipping)"
            )
            return collection.count()

        logger.info(f"Building index for NECB {vintage}...")

        # Load documents from database
        documents = self._load_documents(vintage)
        logger.info(f"Loaded {len(documents)} documents from database")

        # Generate embeddings in batches
        all_embeddings = self._generate_embeddings([doc.content for doc in documents])

        # Add to ChromaDB
        collection.add(
            ids=[doc.id for doc in documents],
            documents=[doc.content for doc in documents],
            metadatas=[doc.metadata for doc in documents],
            embeddings=all_embeddings,
        )

        logger.info(f"✓ Indexed {len(documents)} documents for NECB {vintage}")
        return len(documents)

    def _load_documents(self, vintage: str) -> List[NECBDocument]:
        """Load all sections and tables for a vintage from SQLite."""
        documents = []

        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # Load sections
        cursor.execute(
            """
            SELECT id, section_number, title, content, page_number
            FROM necb_sections
            WHERE vintage = ?
        """,
            (vintage,),
        )

        for row in cursor.fetchall():
            section_id, section_num, title, content, page_num = row

            # Skip empty content
            if not content or not content.strip():
                continue

            doc = NECBDocument(
                id=f"{vintage}_section_{section_id}",
                content=f"Section {section_num}: {title}\n\n{content}",
                metadata={
                    "vintage": vintage,
                    "content_type": "section",
                    "section_number": section_num,
                    "title": title or "",
                    "page_number": page_num or 0,
                },
            )
            documents.append(doc)

        # Load tables
        cursor.execute(
            """
            SELECT t.id, t.table_number, t.title, t.headers, t.page_number,
                   GROUP_CONCAT(r.row_data, '|||') as all_rows
            FROM necb_tables t
            LEFT JOIN necb_table_rows r ON r.table_id = t.id
            WHERE t.vintage = ?
            GROUP BY t.id
        """,
            (vintage,),
        )

        for row in cursor.fetchall():
            table_id, table_num, title, headers_json, page_num, rows_concat = row

            # Parse headers
            try:
                headers = json.loads(headers_json) if headers_json else []
            except json.JSONDecodeError:
                headers = []

            # Parse rows
            rows = []
            if rows_concat:
                for row_json in rows_concat.split("|||"):
                    try:
                        rows.append(json.loads(row_json))
                    except json.JSONDecodeError:
                        continue

            # Skip empty tables
            if not headers and not rows:
                continue

            # Format table as text
            table_text = f"Table {table_num}"
            if title:
                table_text += f": {title}"
            table_text += "\n\n"

            if headers:
                table_text += "Headers: " + " | ".join(str(h) for h in headers) + "\n"

            if rows:
                table_text += f"Data ({len(rows)} rows):\n"
                for i, row_data in enumerate(rows[:10], 1):  # First 10 rows for preview
                    if isinstance(row_data, list):
                        table_text += f"  {i}. " + " | ".join(str(cell) for cell in row_data) + "\n"

                if len(rows) > 10:
                    table_text += f"  ... ({len(rows) - 10} more rows)\n"

            doc = NECBDocument(
                id=f"{vintage}_table_{table_id}",
                content=table_text,
                metadata={
                    "vintage": vintage,
                    "content_type": "table",
                    "table_number": table_num,
                    "title": title or "",
                    "page_number": page_num or 0,
                    "row_count": len(rows),
                },
            )
            documents.append(doc)

        conn.close()

        logger.info(
            f"Loaded {len([d for d in documents if d.metadata['content_type'] == 'section'])} sections, "
            f"{len([d for d in documents if d.metadata['content_type'] == 'table'])} tables"
        )

        return documents

    def _generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings in batches."""
        logger.info(f"Generating embeddings for {len(texts)} documents...")

        # Add e5 prefix for better performance (model-specific)
        if "e5" in self.model_config.get("model_name", "").lower():
            texts = [f"passage: {text}" for text in texts]

        # Generate in batches
        embeddings = self.model.encode(
            texts,
            batch_size=self.batch_size,
            show_progress_bar=True,
            convert_to_numpy=True,
        )

        logger.info(f"✓ Generated {embeddings.shape[0]} embeddings of dimension {embeddings.shape[1]}")

        return embeddings.tolist()

    def get_collection(self, vintage: str) -> chromadb.Collection:
        """Get ChromaDB collection for a vintage."""
        collection_name = f"necb_{vintage}"
        return self.client.get_collection(collection_name)

    def get_stats(self) -> Dict:
        """Get index statistics."""
        stats = {
            "model": self.model_config.get("model_name"),
            "device": self.model_config.get("device"),
            "dimensions": self.model_config.get("dimensions"),
            "chroma_path": str(self.chroma_path),
            "collections": {},
        }

        for collection in self.client.list_collections():
            stats["collections"][collection.name] = {
                "count": collection.count(),
                "metadata": collection.metadata,
            }

        return stats


def main():
    """CLI for building NECB vector index."""
    import sys
    import argparse

    logging.basicConfig(
        level=logging.INFO,
        format="%(message)s",
    )

    parser = argparse.ArgumentParser(description="Build NECB semantic search index")
    parser.add_argument(
        "--db",
        type=Path,
        default=Path("src/bluesky/mcp/data/necb.db"),
        help="Path to necb.db",
    )
    parser.add_argument(
        "--chroma",
        type=Path,
        default=Path("src/bluesky/mcp/data/chroma"),
        help="Path to ChromaDB storage",
    )
    parser.add_argument(
        "--vintages",
        nargs="+",
        default=["2011", "2015", "2017", "2020"],
        help="Vintages to index",
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="Force rebuild (delete existing collections)",
    )
    parser.add_argument(
        "--stats",
        action="store_true",
        help="Show index statistics only",
    )

    args = parser.parse_args()

    # Verify database exists
    if not args.db.exists():
        print(f"❌ Database not found: {args.db}")
        print("Run: python -m bluesky.mcp.scrapers.necb.necb_db_builder")
        sys.exit(1)

    # Initialize indexer
    print("=" * 80)
    print("NECB Vector Index Builder")
    print("=" * 80)

    indexer = NECBVectorIndexer(db_path=args.db, chroma_path=args.chroma)

    if args.stats:
        # Show statistics
        print("\nIndex Statistics:")
        print("-" * 80)
        stats = indexer.get_stats()
        print(f"Model: {stats['model']}")
        print(f"Device: {stats['device']}")
        print(f"Dimensions: {stats['dimensions']}")
        print(f"Storage: {stats['chroma_path']}")
        print(f"\nCollections:")
        for name, info in stats["collections"].items():
            print(f"  {name}: {info['count']} documents")
    else:
        # Build index
        print(f"\nDatabase: {args.db}")
        print(f"ChromaDB: {args.chroma}")
        print(f"Vintages: {', '.join(args.vintages)}")
        print(f"Force rebuild: {args.force}")
        print()

        results = indexer.build_index(vintages=args.vintages, force_rebuild=args.force)

        print("\n" + "=" * 80)
        print("Index Build Complete")
        print("=" * 80)
        print(f"\nResults:")
        for vintage, count in results.items():
            print(f"  NECB {vintage}: {count:,} documents")

        print(f"\n✓ Index saved to {args.chroma}")

    sys.exit(0)


if __name__ == "__main__":
    main()
